{
  "posts": [
    {
      "slug": "01_hello_layout",
      "title": "Hello, Layout! ; Visualizing Memory in CuTe",
      "description": "Understanding CuTe Layouts: how shape and stride turn flat memory into multidimensional grids.",
      "tag": "CUDA",
      "date": "2025-06-15"
    },
    {
      "slug": "02_the_art_of_slicing",
      "title": "The Art of Slicing ; Partitioning Data Across Blocks and Threads",
      "description": "How CuTe's local_tile and local_partition replace manual index math to slice matrices across CTAs and threads.",
      "tag": "CUDA",
      "date": "2025-06-20"
    },
    {
      "slug": "03_the_naive_copy",
      "title": "The Naive Copy ; Scalar vs. Vectorized Memory Movement",
      "description": "Why scalar copies leave 75% of memory bandwidth on the table, and how CuTe's auto-vectorization fixes it.",
      "tag": "CUDA",
      "date": "2026-02-22"
    },
    {
      "slug": "04_the_parallel_copy",
      "title": "The Parallel Copy ; Orchestrating Threads with TiledCopy",
      "description": "How TiledCopy bundles thread layout, copy atoms, and value layout into one declarative object for coordinated, vectorized parallel copies.",
      "tag": "CUDA",
      "date": "2026-02-24"
    },
    {
      "slug": "05_swizzling",
      "title": "Swizzling ; Avoiding Shared Memory Bank Conflicts",
      "description": "How CuTe's Swizzle XORs address bits to eliminate shared memory bank conflicts with a single line of code.",
      "tag": "CUDA",
      "date": "2026-02-26"
    }
  ]
}
